{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Code for CCN 2026 Submission 19: \"An Analysis of Neuroidal Memory Formation within D. melanogaster\""
      ],
      "metadata": {
        "id": "g7AVDiBqAO1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import FlyWire's Connections Dataset\n",
        "The default import assumes that the data is housed on a Google user's cloud drive storage. Please edit the file path to point to the correct location of the CSV file."
      ],
      "metadata": {
        "id": "UvDO4KuP_1Py"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfFCy3DUPZag"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE0_TsXjPrb8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/flywire_connections.csv\"\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icZVORlEdlAf"
      },
      "source": [
        "### Versioning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7eK9lW5dors"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1IkqIb65ev2"
      },
      "source": [
        "\n",
        "Each row contains:\n",
        "-----------------------------------------------\n",
        "* **pre_pt_root_id**: (presynaptic neuron ID)\n",
        "* **post_pt_root_id**: (postsynaptic neuron ID)\n",
        "* **neuropil**: (brain region)\n",
        "* **nt_type**: (Neurotransmitter type) This field labels the predicted neurotransmitter for each synaptic connection (e.g., acetylcholine, GABA, glutamate, etc.)\n",
        "* **syn_count**: number of synapses between this specific pre-post pair in that neuropil\n",
        "\n",
        "So in Codex's neuropils view:\n",
        "--------------------------------\n",
        "* Each row corresponds to a directed connection between two neurons within one neuropil.\n",
        "* syn_count: How many synaptic contact points define that edge. Defines strong vs. weak connections.\n",
        "* Example: A syn_count of 7 means neuron X makes 7 synapses onto neuron Y within the specified neuropil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge_Bb2sQ3yxK"
      },
      "outputs": [],
      "source": [
        "df.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH9orqKHQUSr"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC6TPfaVRncH"
      },
      "outputs": [],
      "source": [
        "df[\"pre_pt_root_id\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZdFXrr6RsQ9"
      },
      "outputs": [],
      "source": [
        "df[\"post_pt_root_id\"].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagJpT0B8Nl7"
      },
      "source": [
        "## Neuropil Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyabSbpQVJGZ"
      },
      "outputs": [],
      "source": [
        "# The regions of the brain\n",
        "# Refers to the brain region where a synapse connects. In flies, brain regions are divided into neuropils—bundled fiber areas like the antennal lobe (AL),\n",
        "# medulla (ME), lobula (LO), mushroom body (MB), central complex, etc.\n",
        "# In Codex, each synapse edge reports the neuropil (region) in which it's found. Edges may span different neuropils, and are grouped by region in the graph.\n",
        "# https://codex.flywire.ai/app/neuropils?dataset=fafb\n",
        "\n",
        "df[\"neuropil\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvOud-VFtHdk"
      },
      "outputs": [],
      "source": [
        "df_long = pd.melt(df, id_vars=\"neuropil\", value_vars=[\"pre_pt_root_id\", \"post_pt_root_id\"],\n",
        "                  var_name=\"role\", value_name=\"pt_root_id\")\n",
        "\n",
        "unique_ids_per_neuropil = df_long.groupby(\"neuropil\")[\"pt_root_id\"].nunique().sort_values()\n",
        "unique_ids_per_neuropil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH3M1wKVtL84"
      },
      "source": [
        "## Start with **Smallest** brain regions.\n",
        "\n",
        "\n",
        "\n",
        "1.   LH_L (5723)\n",
        "2.   OCG (409)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV5GO6Iu4N4V"
      },
      "outputs": [],
      "source": [
        "df_LH_L = df[df[\"neuropil\"]=='LH_L']\n",
        "df_LH_L.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5YMilS2um6S"
      },
      "outputs": [],
      "source": [
        "# In a directed graph adjacency matrix, the row that represents node '720575940608199748' will have 277 nonzero entries.\n",
        "df_LH_L[df_LH_L['pre_pt_root_id'] == 720575940608199748]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFlmi4d_3U1G"
      },
      "outputs": [],
      "source": [
        "# Number of edges/connections in LH_L brain region.\n",
        "df_LH_L.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XsA9NoO7SzU"
      },
      "outputs": [],
      "source": [
        "# Number of unique nodes in the LH_L brain region.\n",
        "\n",
        "# Get unique pre and post node IDs separately\n",
        "pre_nodes = df_LH_L['pre_pt_root_id'].unique()\n",
        "post_nodes = df_LH_L['post_pt_root_id'].unique()\n",
        "\n",
        "# Use set union (mask-based logic)\n",
        "unique_nodes = set(pre_nodes) | set(post_nodes)\n",
        "\n",
        "# Count total unique nodes\n",
        "num_unique_nodes = len(unique_nodes)\n",
        "\n",
        "print(\"Total unique nodes:\", num_unique_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KexwBrd39ip"
      },
      "outputs": [],
      "source": [
        "# unique pre nodes in LH_L\n",
        "df_LH_L[\"pre_pt_root_id\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTPCego44F4S"
      },
      "outputs": [],
      "source": [
        "# unique post nodes in LH_L\n",
        "df_LH_L[\"post_pt_root_id\"].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bpXgo8jgcih"
      },
      "source": [
        "## Create an adjacency graph of the LH_L subgraph connections.\n",
        "(binary, unweighted)\n",
        "\n",
        "*   adj_sparse: CSR sparse adjacency matrix (0/1 entries)\n",
        "*   node_to_idx: Mapping from neuron ID --> matrix row/col index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBuxBv0Qgi86"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "def compute_adj_sparse_mat(df_subgraph, directed=False):\n",
        "  # Get all unique node IDs\n",
        "  nodes = pd.unique(df_subgraph[['pre_pt_root_id', 'post_pt_root_id']].values.ravel())\n",
        "  node_to_idx = {node: i for i, node in enumerate(nodes)}\n",
        "\n",
        "  # Map each ID to its index\n",
        "  row = df_subgraph['pre_pt_root_id'].map(node_to_idx).values\n",
        "  col = df_subgraph['post_pt_root_id'].map(node_to_idx).values\n",
        "\n",
        "  # # Use syn_count as the weight for each edge?\n",
        "  # data = df_subgraph['syn_count'].values>\n",
        "\n",
        "  # Use binary weights: 1 for each edge\n",
        "  data = np.ones_like(row, dtype=int)\n",
        "\n",
        "  # Build sparse adjacency matrix (directed by default)\n",
        "  n = len(nodes)\n",
        "  adj_sparse = coo_matrix((data, (row, col)), shape=(n, n)).tocsr()\n",
        "\n",
        "  # To make the graph UNDIRECTED:\n",
        "  if not directed:\n",
        "    adj_sparse = adj_sparse + adj_sparse.T\n",
        "    adj_sparse.data[:] = 1  # Ensure it's still binary (not 2s)\n",
        "\n",
        "  return adj_sparse, node_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr2KFQbyxArC"
      },
      "source": [
        "#### Sanity check below 3 cells for pre-->post connections in adj matrix representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-22ub-xuJtY"
      },
      "outputs": [],
      "source": [
        "# Print the corresponding adj matrix row index to the original df node id.\n",
        "row_idx = node_to_idx[720575940608199748]\n",
        "row_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7kIH40wuWQg"
      },
      "outputs": [],
      "source": [
        "nonzero_cols = adj_sparse[row_idx].nonzero()[1]\n",
        "print(nonzero_cols.size)\n",
        "print(nonzero_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKNA1WwDw_uq"
      },
      "outputs": [],
      "source": [
        "# Used original df to check for an existing connection between a pre node and a post node, as given by nonzero entries of the generated adj matrix.\n",
        "# MUST map the original id to the zero-index enumerated row of the adj matrix to check against original df.\n",
        "id_to_find = 720575940608199748\n",
        "filtered_rows = df_LH_L[df_LH_L['pre_pt_root_id'] == id_to_find]\n",
        "filtered_rows[filtered_rows[\"post_pt_root_id\"] == 720575940635468991]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWnFOD03zg_-"
      },
      "source": [
        "Confirmed: Node id 720575940608199748 (corresponding to row index 0 in adj matrix) has 277 outgoing edges. This matches the original df 'pre_pt_root_id' == 720575940635468991 277 entries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvoWDCRph0KJ"
      },
      "source": [
        "### Compute Clustering Coefficient and Path Length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctKZ1dAtr86Z"
      },
      "outputs": [],
      "source": [
        "# Calculating Clustering Coeff. with directed graph\n",
        "\n",
        "# Assuming A is a scipy sparse matrix\n",
        "# Need to use a definition of directed clustering for a directed graph, such as Fagiolo's clustering coefficient:\n",
        "# For directed graphs, we need to count directed triangles considering edge directions.\n",
        "import numpy as np\n",
        "from scipy.sparse import issparse\n",
        "\n",
        "def calculate_global_clustering_sparse(A, directed=False) -> float:\n",
        "  if not directed:\n",
        "    # # MAKE ADJ GRAPH UNDIRECTED\n",
        "    A = A.maximum(A.T)  # sparse matrix max method\n",
        "\n",
        "    A2 = A @ A\n",
        "    A2_sum = A2.sum()\n",
        "    A2_trace = A2.diagonal().sum()\n",
        "\n",
        "    # Step 1: Count connected triples = sum(A²) - trace(A²)\n",
        "    k = A2_sum - A2_trace\n",
        "\n",
        "    # Step 2: Calculate Global clustering coefficient, only calculate triangles if triples is nonzero:\n",
        "    if k == 0:\n",
        "      return 0.0\n",
        "\n",
        "    # Step 2.1: Triangle count = trace(A³)\n",
        "    A3 = A2 @ A\n",
        "    A3_trace = A3.diagonal().sum()\n",
        "    return A3_trace / k\n",
        "\n",
        "  else:\n",
        "    # Directed graph clustering coefficient (Fagiolo 2007) https://arxiv.org/pdf/physics/0612169\n",
        "    # Step 1: Binarize adjacency matrix so all edges count as 1.\n",
        "    # Step 2: Symmetrize adjacency A_sym = A + A^T to compute combined in- and out-degrees.\n",
        "    # Step 3: Compute trace(A_sym^3), counting directed triangles.\n",
        "    # Step 4: Compute denominator as sum over nodes of di(di - 1) where di = in_degree + out_degree subtracting bilateral edges (A_bin[i, j] == 1 AND A_bin[j, i] == 1)\n",
        "    # Step 5: Return the ratio as the global clustering coefficient for the directed graph.\n",
        "\n",
        "    # Step 1: Convert to binary adjacency (edges = 1)\n",
        "    A_bin = A.copy()\n",
        "    A_bin.data[:] = 1\n",
        "\n",
        "    # Step 2: Calculate symmetrized adjacency matrix\n",
        "    A_sym = A_bin + A_bin.T\n",
        "\n",
        "    # Step 3: Calculate numerator = trace(A^3)\n",
        "    A3 = A_sym @ A_sym @ A_sym\n",
        "    triangles = A3.diagonal().sum()\n",
        "\n",
        "    # Step 4: Calculate denominator = number of possible triangles\n",
        "    degrees_tot = np.array(A_sym.sum(axis=1)).flatten()  # in-degree + out-degree\n",
        "    A2 = A_bin @ A_bin\n",
        "    num_bilateral_edges = A2.diagonal()\n",
        "    possible_triangles = 2 * ((degrees_tot * (degrees_tot - 1)) - (2 * num_bilateral_edges))\n",
        "\n",
        "    total_possible = possible_triangles.sum()\n",
        "    if total_possible == 0:\n",
        "      return 0.0\n",
        "\n",
        "    # Step 5: Global clustering coefficient\n",
        "    return triangles / total_possible\n",
        "\n",
        "\n",
        "# Calculating Path Length with undirected graph\n",
        "# Use built in scipy.sparse.csgraph shortest path algorithms that are optimized for sparse matrices:\n",
        "from scipy.sparse.csgraph import shortest_path\n",
        "import numpy as np\n",
        "\n",
        "def calculate_path_length_sparse(A, directed=False):\n",
        "  # Ensure adjacency matrix is symmetric if undirected\n",
        "  if not directed:\n",
        "    A = A.maximum(A.T)\n",
        "\n",
        "  # Replace zeros with inf for distances (non-edges)\n",
        "  # shortest_path treats zeros as no edge by default if you pass 'unweighted=False'\n",
        "  # so we can just pass the adjacency matrix directly if it encodes weights properly\n",
        "  # For unweighted graph, set all edges to 1\n",
        "  A.data[:] = 1\n",
        "\n",
        "  # Compute all pairs shortest paths\n",
        "  dist_matrix = shortest_path(A, directed=directed, unweighted=True)\n",
        "\n",
        "  # Mask out infinite distances (no path)\n",
        "  finite_distances = dist_matrix[np.isfinite(dist_matrix) & (dist_matrix != 0)]\n",
        "\n",
        "  # Return average shortest path length (excluding zero distances)\n",
        "  return np.mean(finite_distances)\n",
        "\n",
        "\n",
        "def calculate_degree_distribution(A, directed=True):\n",
        "    if not directed:\n",
        "        # Make adjacency symmetric\n",
        "        A = np.maximum(A, A.T)\n",
        "        # Degree = sum of either rows or columns\n",
        "        degree = np.sum(A, axis=0)\n",
        "        return degree\n",
        "    else:\n",
        "        # For directed: return both in-degree and out-degree\n",
        "        in_degree = np.sum(A, axis=0)   # column sums\n",
        "        out_degree = np.sum(A, axis=1).T  # row sums\n",
        "        return in_degree, out_degree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6wsQf-qiLYK"
      },
      "outputs": [],
      "source": [
        "direct = True\n",
        "\n",
        "# Create adj_sparse\n",
        "adj_sparse, node_to_idx = compute_adj_sparse_mat(df_LH_L, directed=direct)\n",
        "\n",
        "print(\"Shape:\", adj_sparse.shape)\n",
        "print(\"Edges:\", adj_sparse.nnz // 2)  # Undirected, so count each edge once\n",
        "print(\"Edge density:\", adj_sparse.nnz / (adj_sparse.shape[0] ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vIItURaT6AZ"
      },
      "outputs": [],
      "source": [
        "deg_dist_IN, deg_dist_OUT = calculate_degree_distribution(adj_sparse, direct)\n",
        "print(\"In-Degree Distribution: \", deg_dist_IN)\n",
        "print(\"Out-Degree Distribution: \", deg_dist_OUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTt1dgCch9DL"
      },
      "outputs": [],
      "source": [
        "clustCoeff = calculate_global_clustering_sparse(adj_sparse, directed=direct)\n",
        "print(\"Clustering Coefficient: \", clustCoeff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBL_X3asiAq0"
      },
      "outputs": [],
      "source": [
        "characteristic_path_length = calculate_path_length_sparse(adj_sparse, directed=direct)\n",
        "print(\"Characteristic Path Length:\", characteristic_path_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JclXkVDTkmhB"
      },
      "source": [
        "#### Find the average degree of the graph (directed / undirected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5IrrsN1lsVK"
      },
      "outputs": [],
      "source": [
        "# Assume A is a sparse adjacency matrix (CSR format)\n",
        "# Directed graph: row sums = out-degree, col sums = in-degree\n",
        "def compute_graph_average_degree(A, directed=False):\n",
        "  if directed:\n",
        "    total_in_deg = A.sum(axis=0).A1.sum()\n",
        "    total_out_deg = A.sum(axis=1).A1.sum()\n",
        "    # These should be equal for any directed graph\n",
        "    assert total_in_deg == total_out_deg\n",
        "\n",
        "    avg_total_degree_d = (total_in_deg + total_out_deg) / A.shape[0]\n",
        "    # Check the average total degrees of a node to be within bounds\n",
        "    # Since the sum of in-degrees equals the sum of out-degrees in a directed graph (each edge contributes one to each), we are effectively computing:\n",
        "    # avg_total_degree = 2 * total_edges / n\n",
        "    assert avg_total_degree_d <= 2 * (A.shape[0] - 1)\n",
        "\n",
        "    return avg_total_degree_d\n",
        "\n",
        "  else:\n",
        "    # Undirected\n",
        "    degrees = A.sum(axis=1)  # or axis=0\n",
        "    avg_degree = degrees.mean()\n",
        "    return avg_degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZLWfAWynntW"
      },
      "outputs": [],
      "source": [
        "# Compute Directed average degree of graph\n",
        "print(\"Average Degree:\", compute_graph_average_degree(adj_sparse, directed=direct))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2OVW6lfih9G"
      },
      "source": [
        "### Compute Small-World Threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXgjJSuXi5_6"
      },
      "outputs": [],
      "source": [
        "def compute_small_world_threshold(C_g, L_g, C_rand, L_rand):\n",
        "  return (C_g / C_rand) / (L_g / L_rand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGLUbXXOilkO"
      },
      "outputs": [],
      "source": [
        "# import C_rand, L_rand values from randGraph.ipynb\n",
        "C_rand = 0.008435302420417078\n",
        "L_rand = 2.6529490970869216\n",
        "sigma = compute_small_world_threshold(clustCoeff, characteristic_path_length, C_rand, L_rand)\n",
        "print(sigma)\n",
        "\n",
        "if sigma > 1:\n",
        "  print(\"Small world found!\")\n",
        "else:\n",
        "  print(\"Not a small world...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhR_gQFHqIPL"
      },
      "source": [
        "Through the small-world threshold calculation performed above on the LH_L region of the Drosophila brain, we may see that it is a small-world graph. This is defined by a high clustering coefficient and a low characteristic path length, as compared to a random GNP graph with the same number of nodes and average degree as the measured biological subgraph LH_L."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW7Wn_O5wrXl"
      },
      "source": [
        "## OCG Brain Region Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keaUVy4xu9dp"
      },
      "outputs": [],
      "source": [
        "###### OCG #########\n",
        "df_OCG = df[df[\"neuropil\"]=='OCG']\n",
        "direct_OCG = True\n",
        "\n",
        "# Create Sparse Adj Matrix\n",
        "adj_sparse_OCG, node_to_idx_OCG = compute_adj_sparse_mat(df_OCG, directed=direct_OCG)\n",
        "\n",
        "deg_dist_OCG_IN, deg_dist_OCG_OUT = calculate_degree_distribution(adj_sparse_OCG, direct_OCG)\n",
        "print(\"In-Degree Distribution: \", np.mean(deg_dist_OCG_IN))\n",
        "print(\"Out-Degree Distribution: \", np.mean(deg_dist_OCG_OUT))\n",
        "\n",
        "# Compute Clustering Coefficient\n",
        "clustCoeff_OCG = calculate_global_clustering_sparse(adj_sparse_OCG, directed=direct_OCG)\n",
        "print(\"Clustering Coefficient: \", clustCoeff_OCG)\n",
        "\n",
        "# Compute Characteristic Path Length\n",
        "characteristic_path_length_OCG = calculate_path_length_sparse(adj_sparse_OCG, directed=direct_OCG)\n",
        "print(\"Path Length: \", characteristic_path_length_OCG)\n",
        "\n",
        "graph_average_degree_OCG = compute_graph_average_degree(adj_sparse_OCG, directed=direct_OCG)\n",
        "# Compute Average Degree of Graph\n",
        "print(\"Average Degree of Graph:\", graph_average_degree_OCG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFFzDUf5vHnU"
      },
      "outputs": [],
      "source": [
        "###### OCG #########\n",
        "\n",
        "# import C_rand, L_rand values from randGraph.ipynb\n",
        "C_rand_OCG = 0.030600189035916825\n",
        "L_rand_OCG = 2.6501989548875784\n",
        "sigma_OCG = compute_small_world_threshold(clustCoeff_OCG, characteristic_path_length_OCG, C_rand_OCG, L_rand_OCG)\n",
        "print(sigma_OCG)\n",
        "\n",
        "if sigma_OCG > 1:\n",
        "  print(\"Small world found!\")\n",
        "else:\n",
        "  print(\"Not a small world...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8lYgr2s8VoQ"
      },
      "source": [
        "#### nt_type Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tnW30NDVM_F"
      },
      "outputs": [],
      "source": [
        "# Neurotransmitter type: This field labels the predicted neurotransmitter for each synaptic connection (e.g., acetylcholine, GABA, glutamate, etc.)\n",
        "\n",
        "df[\"nt_type\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8SHJT9A8Zeh"
      },
      "source": [
        "#### **Overall connections**: Find the number of times a neuron 'pre_pt_root_id' appears in 'post_pt_root_id'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rak8M2LRPsn3"
      },
      "outputs": [],
      "source": [
        "# Find the number of times a neuron 'pre_pt_root_id' appears in 'post_pt_root_id'\n",
        "\n",
        "# Drop duplicates from 'pre_pt_root_id'\n",
        "unique_col1 = df['pre_pt_root_id'].drop_duplicates()\n",
        "\n",
        "# Count occurrences in 'post_pt_root_id'\n",
        "col2_counts = df['post_pt_root_id'].value_counts()\n",
        "\n",
        "# Map counts to the unique col1 values\n",
        "result = pd.DataFrame({\n",
        "    'pre_pt_root_id': unique_col1,\n",
        "    'count': unique_col1.map(col2_counts).fillna(0).astype(int)\n",
        "})\n",
        "\n",
        "result.sort_values(by='count', ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo6eRuiaj-Fk"
      },
      "source": [
        "# JOIN Algorithm on Brain Region Subgraphs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5h7SXOuk8wE"
      },
      "source": [
        "firing_nodes = a binary vector or list of nodes currently \"firing\"\n",
        "\n",
        "For each node j, count how many firing nodes i have A[i, j] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Vhag3IakFcX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# A: scipy.sparse matrix (n x n), A directed binary adjacency matrix.\n",
        "# firing_nodes: array of indices that are currently firing\n",
        "# n_nodes: number of nodes in graph\n",
        "# k_s: firing threshold for the graph\n",
        "# Returns: np.array of counts of firing inputs per node\n",
        "# Firing from A directly to C, we should measure and allow recurrence up to 10% of mem_size.\n",
        "def count_firing_inputs(A, firing_nodes, k_s, n_nodes=None):\n",
        "  # Recurrence tolerance = 10% of mem_size\n",
        "  RECURRENCE_TOLERANCE_THRESHOLD = 0.9\n",
        "  if n_nodes is None:\n",
        "    n_nodes = A.shape[0]\n",
        "\n",
        "  # Create a binary firing vector (1 where node is firing)\n",
        "  firing_vector = np.zeros(n_nodes, dtype=int)\n",
        "  # Set the mode of these nodes to 'firing'\n",
        "  firing_vector[firing_nodes] = 1\n",
        "\n",
        "  # Compute number of firing inputs for each node (incoming edges from firing nodes)\n",
        "  firing_input_counts = A.T @ firing_vector  # result is dense np.array\n",
        "  firing_input_counts = np.array(firing_input_counts).flatten()\n",
        "\n",
        "  # Threshold: fire if count >= k_s\n",
        "  next_firing_nodes = np.where(firing_input_counts >= k_s)[0]\n",
        "\n",
        "  # Removed original firing nodes to measure overlap between memA and created memC\n",
        "  # If the overlap between memA and resultant memC is greater than 10%, then we remove those overlapped nodes from memC, due to recurrence.\n",
        "  next_firing_nodes_without_overlap = np.setdiff1d(next_firing_nodes, firing_nodes)\n",
        "  if len(next_firing_nodes) > 0 and len(next_firing_nodes_without_overlap) / len(next_firing_nodes) < RECURRENCE_TOLERANCE_THRESHOLD:\n",
        "    next_firing_nodes = next_firing_nodes_without_overlap\n",
        "\n",
        "  # Number of firing nodes that meet the threshold for child memory creation:\n",
        "  num_meeting_threshold = len(next_firing_nodes)\n",
        "\n",
        "  return next_firing_nodes, num_meeting_threshold\n",
        "\n",
        "\n",
        "# k_s = median value of subgraph.syn_count\n",
        "# Synapse Count in a connection represents the number of synapses in a connectio; otherwise described as\n",
        "# axon-->dendrite connections.\n",
        "# Here, we use the synapse count to average over the number of connections to find a median connection strength.\n",
        "def compute_k_s_threshold(df_subgraph) -> int:\n",
        "  return df_subgraph['syn_count'].median()\n",
        "\n",
        "import random\n",
        "# Set Global Seed\n",
        "random.seed(0)\n",
        "# node_to_idx_dict: Dictionary that maps pre, post node id's --> adj sparse matrix indices.\n",
        "# mem_size: desired memory size\n",
        "def generate_random_indices_for_memory(node_to_idx_dict, mem_size):\n",
        "  upper_range_of_indices = len(node_to_idx_dict)\n",
        "  memory_indices = random.sample(range(0, upper_range_of_indices), mem_size)\n",
        "  return memory_indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXhVPez0mFMI"
      },
      "source": [
        "#### Compute the k_s threshold for a brain region, and simulate firing a random subset of nodes to represent Memory A. The connected nodes will also fire into Memory C if they meet the k_s threshold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i8C-uD5Wo8X"
      },
      "outputs": [],
      "source": [
        "# Compute the synapse strength to find the k_s, firing threshold\n",
        "k_s_LH_L = compute_k_s_threshold(df_LH_L)\n",
        "print(k_s_LH_L)\n",
        "\n",
        "\n",
        "# Memory A\n",
        "# Grab a random subset of nodes through the node_to_idx dictionary, as represented by their indices.\n",
        "memory_A_size = 100\n",
        "memory_A = generate_random_indices_for_memory(node_to_idx, memory_A_size)\n",
        "\n",
        "# Fire Memory A, and observe the number of firing nodes that create memory C. Also included: the indices of memory C.\n",
        "activated_nodes, memory_C_size = count_firing_inputs(adj_sparse, memory_A, k_s_LH_L)\n",
        "print(\"Size of Memory C: \", memory_C_size)\n",
        "print(\"Memory C: \", activated_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spyMCiQOlhiG"
      },
      "source": [
        "### Plot varying sizes of Memory A to fire and create Memory C in a brain region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDSupUeZe0eB"
      },
      "outputs": [],
      "source": [
        "# Try varying sizes of memory A\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Plots memA against fired memC over a range of sizes for memA\n",
        "def plot_memA_firing_against_memC_creation_single_trials(memory_A_sizes, memory_C_sizes, node_to_idx_dict, adj_matrix, k_s, memory_A_size_MIN, memory_A_size_MAX, brain_region, increment=10, num_trials=20):\n",
        "  for size in range(memory_A_size_MIN, memory_A_size_MAX, increment):\n",
        "    memory_A_sizes.append(size)\n",
        "    # Random subset of nodes for Memory A\n",
        "    memory_A = generate_random_indices_for_memory(node_to_idx_dict, size)\n",
        "\n",
        "    # Fire memory A\n",
        "    activated_nodes, memory_C_size = count_firing_inputs(adj_matrix, memory_A, k_s)\n",
        "\n",
        "    memory_C_sizes.append(memory_C_size)\n",
        "    print(f\"Memory A size: {size}, Memory C size: {memory_C_size}\")\n",
        "\n",
        "# Plots several rounds of trials of: memA against fired memC over a range of sizes for memA\n",
        "def plot_memA_firing_against_memC_creation(memory_A_sizes, memory_C_sizes, node_to_idx_dict, adj_matrix, k_s, memory_A_size_MIN, memory_A_size_MAX, brain_region, graphType, increment=10, num_trials=20):\n",
        "  num_points = ((memory_A_size_MAX - memory_A_size_MIN) // increment) + 1\n",
        "  memory_C_sizes = [0] * num_points\n",
        "  memory_A_sizes = list(range(memory_A_size_MIN, memory_A_size_MAX + 1, increment))\n",
        "  for i in range(num_trials):\n",
        "    for idx, size in enumerate(memory_A_sizes):\n",
        "      # Random subset of nodes for Memory A\n",
        "      memory_A = generate_random_indices_for_memory(node_to_idx_dict, size)\n",
        "\n",
        "      # Fire memory A\n",
        "      activated_nodes, memory_C_size = count_firing_inputs(adj_matrix, memory_A, k_s)\n",
        "\n",
        "      memory_C_sizes[idx] += memory_C_size\n",
        "      print(f\"Memory A size: {size}, Memory C size: {memory_C_size}, size:{size}, index:{size//increment}, num_points: {num_points}\")\n",
        "\n",
        "  # Average out mem_C size values over trials\n",
        "  memory_C_sizes = [x / num_trials for x in memory_C_sizes]\n",
        "\n",
        "  # Plot the relationship\n",
        "  plt.figure(figsize=(6,4))\n",
        "  plt.plot(memory_A_sizes, memory_C_sizes, marker='o', color='black', linestyle='-', markerfacecolor='gray')\n",
        "  plt.xlabel(\"Memory A Size\")\n",
        "  plt.ylabel(\"Memory C Size\", rotation=0, labelpad=40)\n",
        "  plt.title(f\"Memory A Size vs Memory C Size of Brain Region {brain_region}\")\n",
        "  plt.grid(False)\n",
        "  plt.savefig(f\"stability{graphType}.pdf\", format='pdf', bbox_inches='tight')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Plots several rounds of trials of: memA against fired memC over a range of sizes for memA, with standard deviation error bars\n",
        "def plot_memA_firing_against_memC_creation_with_error_bars(memory_A_sizes, memory_C_sizes, node_to_idx_dict, adj_matrix, k_s, memory_A_size_MIN, memory_A_size_MAX, brain_region, increment=10, num_trials=20):\n",
        "  # Setup: discrete bins of memory A\n",
        "  memory_A_sizes = list(range(memory_A_size_MIN, memory_A_size_MAX + 1, increment))\n",
        "  num_points = len(memory_A_sizes)\n",
        "\n",
        "  # Collect trial results (list of lists)\n",
        "  all_trials = [[] for _ in range(num_points)]\n",
        "\n",
        "  # Run trials num_trials times\n",
        "  for i in range(num_trials):\n",
        "    for idx, size in enumerate(memory_A_sizes):\n",
        "      # Random subset of nodes for Memory A\n",
        "      memory_A = generate_random_indices_for_memory(node_to_idx_dict, size)\n",
        "      # Fire memory A\n",
        "      activated_nodes, memory_C_size = count_firing_inputs(adj_matrix, memory_A, k_s)\n",
        "      # add memory_C size to the respective list\n",
        "      all_trials[idx].append(memory_C_size)\n",
        "\n",
        "  # Compute mean and standard deviation across trials\n",
        "  memory_C_sizes = [np.mean(vals) for vals in all_trials]\n",
        "  memory_C_stds  = [np.std(vals) for vals in all_trials]  # or use np.std(vals)/np.sqrt(num_trials) for standard error\n",
        "\n",
        "  # Plot the mean line for error regions\n",
        "  plt.plot(\n",
        "      memory_A_sizes,\n",
        "      memory_C_sizes,\n",
        "      '-o',\n",
        "      color='black',\n",
        "      markerfacecolor='gray',\n",
        "      label='Mean ± Std'\n",
        "  )\n",
        "\n",
        "  # Add the shaded error region\n",
        "  plt.fill_between(\n",
        "      memory_A_sizes,\n",
        "      np.array(memory_C_sizes) - np.array(memory_C_stds),\n",
        "      np.array(memory_C_sizes) + np.array(memory_C_stds),\n",
        "      color='coral',\n",
        "      alpha=0.3  # transparency level\n",
        "  )\n",
        "\n",
        "  plt.xlabel(\"Memory A Size\")\n",
        "  plt.ylabel(\"Memory C Size\", rotation=0, labelpad=40)\n",
        "  plt.title(f\"Memory A Size vs Memory C Size of Brain Region {brain_region}\")\n",
        "  plt.grid(False)\n",
        "  plt.legend()\n",
        "  plt.savefig(\"stabilityOCG.pdf\", format='pdf', bbox_inches='tight')\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9Ftxkn3wmUd"
      },
      "source": [
        "### LH_L: Plot varying sizes of Memory A to fire and create Memory C in a brain region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcikCJ-KwTUy"
      },
      "outputs": [],
      "source": [
        "# Compute the synapse strength to find the k_s, firing threshold\n",
        "k_s_LH_L = compute_k_s_threshold(df_LH_L)\n",
        "print(k_s_LH_L)\n",
        "\n",
        "# Generate memory A and fire to get memory C, then plot.\n",
        "brain_region_name = \"LH_L\"\n",
        "memory_A_size_MIN = 1\n",
        "memory_A_size_MAX = 1000  # different A sizes to test\n",
        "k_s = k_s_LH_L\n",
        "memory_A_sizes = []  # for plotting\n",
        "memory_C_sizes = []  # for plotting\n",
        "node_to_idx_dict = node_to_idx\n",
        "adj_matrix = adj_sparse\n",
        "increment = 20\n",
        "num_trials = 1\n",
        "\n",
        "# Plot varying sizes of fired nodes in Memory A against firing nodes of created Memory C\n",
        "plot_memA_firing_against_memC_creation(memory_A_sizes, memory_C_sizes, node_to_idx_dict, adj_matrix, k_s,\n",
        "                                       memory_A_size_MIN, memory_A_size_MAX, brain_region_name, \"LHL\", increment, num_trials)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbz6kjoKuS7J"
      },
      "source": [
        "### OCG: Plot varying sizes of Memory A to fire and create Memory C in a brain region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENp9eoMNuqwM"
      },
      "outputs": [],
      "source": [
        "# Compute the synapse strength to find the k_s, firing threshold\n",
        "k_OCG = compute_k_s_threshold(df_OCG)\n",
        "print(k_OCG)\n",
        "\n",
        "# Generate memory A and fire to get memory C, then plot.\n",
        "brain_region_name_OCG = \"OCG\"\n",
        "memory_A_size_MIN_OCG = 100\n",
        "memory_A_size_MAX_OCG = 125  # different A sizes to test\n",
        "k_s_OCG = k_OCG\n",
        "memory_A_sizes_OCG = []  # for plotting\n",
        "memory_C_sizes_OCG = []  # for plotting\n",
        "node_to_idx_dict_OCG = node_to_idx_OCG\n",
        "adj_matrix_OCG = adj_sparse_OCG\n",
        "increment_OCG = 1\n",
        "num_trials_OCG = 30\n",
        "\n",
        "# Plot varying sizes of fired nodes in Memory A against firing nodes of created Memory C\n",
        "plot_memA_firing_against_memC_creation(memory_A_sizes_OCG, memory_C_sizes_OCG, node_to_idx_dict_OCG, adj_matrix_OCG, k_s_OCG, memory_A_size_MIN_OCG,\n",
        "                                       memory_A_size_MAX_OCG, brain_region_name_OCG, \"OCG\", increment_OCG, num_trials_OCG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wt38cfcKmKP"
      },
      "outputs": [],
      "source": [
        "plot_memA_firing_against_memC_creation_with_error_bars(memory_A_sizes_OCG, memory_C_sizes_OCG, node_to_idx_dict_OCG, adj_matrix_OCG, k_s_OCG, memory_A_size_MIN_OCG,\n",
        "                                       memory_A_size_MAX_OCG, brain_region_name_OCG, increment_OCG, num_trials_OCG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7L3GcjSfbWF"
      },
      "source": [
        "## Stability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3IBh3v1mUvm"
      },
      "source": [
        "### Fire Memory A and Memory B together to create C in a single brain region (Shared Representation).\n",
        "\n",
        "\n",
        "For the shared representation, **the size of union(A,B) should be of size 2r - (r^2 / n)**, where the resulting memory C should be about **r**.\n",
        "\n",
        "*   **(r^2 / n)** = intersection of A and B\n",
        "*   **r** = num of nodes of mem A, B, C\n",
        "*   **n** = total num of nodes in graph\n",
        "*   As A and B need to be fired at the same time, we can simply take a single, combined set of size 2r - (r^2 / n) and fire that in a 1-step JOIN to get memory C.\n",
        "\n",
        "1.   Memory A: random subset of nodes in a brain region, overlap with B is ok.\n",
        "2.   Memory B: random subset of nodes in a brain region, overlap with A is ok.\n",
        "3.   Memroy C: resulting subset of nodes in a brain region that meet the firing threshold, k.\n",
        "\n",
        "If the Interference Tolerance is breached, take only unique nodes for memory C.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Opj3xU38SX6"
      },
      "outputs": [],
      "source": [
        "# Plots several rounds of trials of: memA against fired memC over a range of sizes for memA, with error regions\n",
        "def plot_memAB_firing_against_memC_creation_with_error_bars(memory_AB_sizes, memory_C_sizes, node_to_idx_dict, adj_matrix, k_s, memory_AB_size_MIN, memory_AB_size_MAX, brain_region, increment=10, num_trials=20):\n",
        "  # Setup: discrete bins of memory A\n",
        "  memory_AB_sizes = list(range(memory_AB_size_MIN, memory_AB_size_MAX + 1, increment))\n",
        "  num_points = len(memory_AB_sizes)\n",
        "\n",
        "  # Collect trial results (list of lists)\n",
        "  all_trials = [[] for _ in range(num_points)]\n",
        "\n",
        "  # Run trials num_trials times\n",
        "  for i in range(num_trials):\n",
        "    for idx, size in enumerate(memory_AB_sizes):\n",
        "      # Random subset of nodes for Memory AB\n",
        "      memory_AB = generate_random_indices_for_memory(node_to_idx_dict, size)\n",
        "      # Fire memory AB\n",
        "      activated_nodes, memory_C_size = count_firing_inputs(adj_matrix, memory_AB, k_s)\n",
        "      # add memory_C size to the respective list\n",
        "      all_trials[idx].append(memory_C_size)\n",
        "\n",
        "  # Compute Mean and Standard Deveiation across all trials\n",
        "  memory_C_sizes = [np.mean(vals) for vals in all_trials]\n",
        "  memory_C_stds  = [np.std(vals) for vals in all_trials]  # or use np.std(vals)/np.sqrt(num_trials) for standard error\n",
        "\n",
        "  # Plot the mean line\n",
        "  plt.plot(\n",
        "      memory_AB_sizes,\n",
        "      memory_C_sizes,\n",
        "      '-o',\n",
        "      color='black',\n",
        "      markerfacecolor='gray',\n",
        "      label='Mean ± Std'\n",
        "  )\n",
        "\n",
        "  # Add the shaded error region\n",
        "  plt.fill_between(\n",
        "      memory_AB_sizes,\n",
        "      np.array(memory_C_sizes) - np.array(memory_C_stds),\n",
        "      np.array(memory_C_sizes) + np.array(memory_C_stds),\n",
        "      color='tomato',\n",
        "      alpha=0.3  # transparency level\n",
        "  )\n",
        "\n",
        "  plt.xlabel(\"Union of Memory A and B Size\")\n",
        "  plt.ylabel(\"Memory C Size\", rotation=0, labelpad=40)\n",
        "  plt.title(f\"Union of Memory A and Memory B Size vs. Memory C Size of Brain Region {brain_region}\")\n",
        "  plt.grid(False)\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots several rounds of trials of: memA against fired memC over a range of sizes for memA, with standard deviation error bars\n",
        "def plot_memAB_firing_against_memC_creation_with_error_bars_and_UNION_sizes(r_min, r_max, n, memory_AB_sizes, memory_C_sizes, node_to_idx_dict, adj_matrix, k_s, memory_AB_size_MIN, memory_AB_size_MAX, brain_region, graphType, increment=10, num_trials=20):\n",
        "  # Setup: discrete bins of memory A\n",
        "  memory_AB_sizes = list(range(memory_AB_size_MIN, memory_AB_size_MAX + 1, increment))\n",
        "  num_points = len(memory_AB_sizes)\n",
        "\n",
        "  # Collect trial results (list of lists)\n",
        "  all_trials = [[] for _ in range(num_points)]\n",
        "\n",
        "  # Run trials num_trials times\n",
        "  for i in range(num_trials):\n",
        "    for idx, size in enumerate(memory_AB_sizes):\n",
        "      # Random subset of nodes for Memory AB\n",
        "      memory_AB = generate_random_indices_for_memory(node_to_idx_dict, size)\n",
        "      # Fire memory AB\n",
        "      activated_nodes, memory_C_size = count_firing_inputs(adj_matrix, memory_AB, k_s)\n",
        "      # add memory_C size to the respective list\n",
        "      all_trials[idx].append(memory_C_size)\n",
        "\n",
        "  # Compute Mean and Standard Deveiation across all trials\n",
        "  memory_C_sizes = [np.mean(vals) for vals in all_trials]\n",
        "  memory_C_stds  = [np.std(vals) for vals in all_trials]  # or use np.std(vals)/np.sqrt(num_trials) for standard error\n",
        "\n",
        "  # Plot the mean line\n",
        "  plt.plot(\n",
        "      memory_AB_sizes,\n",
        "      memory_C_sizes,\n",
        "      '-o',\n",
        "      color='black',\n",
        "      markerfacecolor='gray',\n",
        "      label='Mean ± Std'\n",
        "  )\n",
        "\n",
        "  # Add the shaded error region\n",
        "  plt.fill_between(\n",
        "      memory_AB_sizes,\n",
        "      np.array(memory_C_sizes) - np.array(memory_C_stds),\n",
        "      np.array(memory_C_sizes) + np.array(memory_C_stds),\n",
        "      color='tomato',\n",
        "      alpha=0.3  # transparency level\n",
        "  )\n",
        "\n",
        "  # ---- Add UNION sizes y = 2r - (r^2 / n) for same r range ----\n",
        "  r_sizes = list(range(r_min, r_max + 1, increment))\n",
        "  UNION_sizes = [2 * r_size - (r_size**2) // n for r_size in r_sizes]\n",
        "\n",
        "  plt.plot(  # flip r_sizes and Union sizes so that they overlay on graph as (Union sizes, r_size created)\n",
        "      UNION_sizes,\n",
        "      r_sizes,\n",
        "      '--',\n",
        "      label=r\"UNION sizes vs. r sizes $y = 2r - \\frac{r^{2}}{n}$\"\n",
        "  )\n",
        "\n",
        "  plt.xlabel(\"Union of Memory A and B Size\")\n",
        "  plt.ylabel(\"Memory C Size\", rotation=0, labelpad=40)\n",
        "  plt.title(f\"Union of Memory A and Memory B Size vs. Memory C Size of Brain Region {brain_region}\")\n",
        "  plt.grid(False)\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.savefig(f\"stabilityUNIONOCG{graphType}.pdf\", format='pdf', bbox_inches='tight')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "KvtRc31LWimF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK53S-vpmZh5"
      },
      "outputs": [],
      "source": [
        "# Jointly fire Mem A and Mem B together to create Mem C. Use a one-step method by approximating the size union(A,B).\n",
        "def JOIN_Stability_Testing(df_subgraph, r_min, r_max, n, brain_region_name, node_to_idx_dict, adj_matrix, graphType):\n",
        "  # Compute the size of Union(A,B) for r_min and r_max, respectively. 2 different sizes are calculated for a range of firing values.\n",
        "  union_AB_min_size = 2 * r_min - ((r_min ** 2) // n)\n",
        "  union_AB_max_size = 2 * r_max - ((r_max ** 2) // n)\n",
        "\n",
        "  # Compute the synapse strength to find the k_s, firing threshold\n",
        "  k_s_subgraph = compute_k_s_threshold(df_subgraph)\n",
        "  print(k_s_subgraph)\n",
        "\n",
        "  # Generate memory A and fire to get memory C, then plot.\n",
        "  k_s = k_s_subgraph\n",
        "\n",
        "  memory_AB_size_MIN = union_AB_min_size  # different Union(A,B) sizes to test\n",
        "  memory_AB_size_MAX = union_AB_max_size  # different Union(A,B) sizes to test\n",
        "  memory_AB_sizes = []  # for plotting\n",
        "  memory_C_sizes = []  # for plotting\n",
        "\n",
        "  increment = 1\n",
        "  num_trials = 30\n",
        "\n",
        "  # Plot varying sizes of fired nodes in Memory A against firing nodes of created Memory C\n",
        "  plot_memAB_firing_against_memC_creation_with_error_bars_and_UNION_sizes(r_min, r_max, n, memory_AB_sizes, memory_C_sizes, node_to_idx_dict, adj_matrix, k_s, memory_AB_size_MIN,\n",
        "                                        memory_AB_size_MAX, brain_region_name, graphType, increment, num_trials)\n",
        "\n",
        "\n",
        "# params specifically needed for 2-memory firing.\n",
        "r_min_per_mem_OCG = 100\n",
        "r_max_per_mem_OCG = 140\n",
        "n_OCG = 409\n",
        "# other params needed\n",
        "df_OCG = df[df[\"neuropil\"]=='OCG']\n",
        "brain_region_name_OCG = \"OCG\"\n",
        "node_to_idx_dict_OCG = node_to_idx_OCG\n",
        "adj_matrix_OCG = adj_sparse_OCG\n",
        "\n",
        "# Fire memory A and memory B together, with varying sizes of 100-125 nodes each.\n",
        "JOIN_Stability_Testing(df_OCG, r_min_per_mem_OCG, r_max_per_mem_OCG, n_OCG, brain_region_name_OCG, node_to_idx_dict_OCG, adj_matrix_OCG, \"fullUnionRange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the best UNION range is between [200, 220] where the anticipated sizes fall in between a standard deviation of the error region.\n"
      ],
      "metadata": {
        "id": "m-MFfybLZRCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params specifically needed for 2-memory firing.\n",
        "r_min_per_mem_OCG = 115\n",
        "r_max_per_mem_OCG = 132\n",
        "n_OCG = 409\n",
        "# other params needed\n",
        "df_OCG = df[df[\"neuropil\"]=='OCG']\n",
        "brain_region_name_OCG = \"OCG\"\n",
        "node_to_idx_dict_OCG = node_to_idx_OCG\n",
        "adj_matrix_OCG = adj_sparse_OCG\n",
        "\n",
        "# Fire memory A and memory B together, with varying sizes of 100-125 nodes each.\n",
        "JOIN_Stability_Testing(df_OCG, r_min_per_mem_OCG, r_max_per_mem_OCG, n_OCG, brain_region_name_OCG, node_to_idx_dict_OCG, adj_matrix_OCG, \"partialUnionRange\")"
      ],
      "metadata": {
        "id": "-NGCrnRRZQBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuThedOEfeWd"
      },
      "source": [
        "## Capacity\n",
        "\n",
        "Capacity Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAZIm2QoSrVH"
      },
      "source": [
        "Run Join again and again creating Memory C1, C2, C3, ... [D]\n",
        "Every time we run and create Memory C, run a check to see how much interference there is between resulting Memory D's.\n",
        "\n",
        "Interference is what we are trying to find out! (What % of nodes are interfering with others, how many memories can we create using new A and B each generation?)\n",
        "\n",
        "*A node can belong to many memories.*\n",
        "\n",
        "**Local Interference Rate:** Check every individual node in C and in all other memories D and if more than 50% of the nodes in D are shared with C, then those 2 memories are considered to be interfering.\n",
        "* (# of interfering memories) / (# of all memories) > 10% of preexisting memories\n",
        "* Interference --> can not form the new memory, designate the graph to be 'at capacity'.\n",
        "\n",
        "**Global Interference:** If **more than 10% of all nodes in the whole graph** (OCG) belong to 2+ memories --> Interference Found!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjZWJHwWZFkF"
      },
      "outputs": [],
      "source": [
        "# Jointly fire Mem A and Mem B together to create Mem C. Use a one-step method by approximating the size union(A,B).\n",
        "def JOIN_Capacity_Simulation(df_subgraph, r_min, r_max, n, brain_region_name, node_to_idx_dict, adj_matrix):\n",
        "  # Compute the size of Union(A,B) for r_min and r_max, respectively. 2 different sizes are calculated for a range of firing values.\n",
        "  union_AB_min_size = 2 * r_min - ((r_min ** 2) // n) - 1\n",
        "  union_AB_max_size = 2 * r_max - ((r_max ** 2) // n) - 1\n",
        "\n",
        "  # Compute the synapse strength to find the k_s, firing threshold\n",
        "  k_s_subgraph = compute_k_s_threshold(df_subgraph)\n",
        "  print(k_s_subgraph)\n",
        "\n",
        "  # Generate memory A and fire to get memory C, then plot.\n",
        "  k_s = k_s_subgraph\n",
        "\n",
        "  memory_AB_size_MIN = union_AB_min_size  # different Union(A,B) sizes to test\n",
        "  memory_AB_size_MAX = union_AB_max_size  # different Union(A,B) sizes to test\n",
        "  memory_AB_sizes = []  # for plotting\n",
        "  stable_memories_sizes = []  # for plotting\n",
        "\n",
        "  increment = 1\n",
        "\n",
        "  # Plot varying sizes of fired nodes in Memory A against firing nodes of created Memory C\n",
        "  memory_AB_sizes, stable_memories_sizes, stable_memories_stds = plot_memAB_firing_against_memC_Capacity_Testing_with_error_bars(memory_AB_sizes, stable_memories_sizes, node_to_idx_dict, adj_matrix, k_s, memory_AB_size_MIN,\n",
        "                                        memory_AB_size_MAX, brain_region_name, \"OCG\", increment)\n",
        "  return memory_AB_sizes, stable_memories_sizes, stable_memories_stds\n",
        "\n",
        "\n",
        "# Plots several rounds of trials of: memA against fired memC over a range of sizes for memA, with standard deviation error bars\n",
        "def plot_memAB_firing_against_memC_Capacity_Testing_with_error_bars(memory_AB_sizes, stable_memories_sizes, node_to_idx_dict, adj_matrix, k_s, memory_AB_size_MIN, memory_AB_size_MAX, brain_region, graphType, increment=1, num_trials=30):\n",
        "  # Setup Local and Global Interference expectations\n",
        "  LOCAL_INTERFERENCE_THRESHOLD = 0.5\n",
        "  GLOBAL_INTERFERENCE_THRESHOLD = 0.1\n",
        "\n",
        "  # Setup: discrete bins of memory AB\n",
        "  memory_AB_sizes = list(range(memory_AB_size_MIN, memory_AB_size_MAX + 1, increment))\n",
        "\n",
        "  # Store num stable memories per Union(A,B) size, across num_trials\n",
        "  stable_memories_per_AB_size = [[] for _ in range(len(memory_AB_sizes))]\n",
        "  Global_interference_per_AB_size = [[] for _ in range(len(memory_AB_sizes))]\n",
        "\n",
        "  for i in range(num_trials):\n",
        "    # Collect Memory results (list of lists) and Global Interference results\n",
        "    num_points = len(memory_AB_sizes)\n",
        "    all_trials = [[] for _ in range(num_points)]\n",
        "    Global_Interference_Levels_per_AB_Size = [0.0] * num_points\n",
        "\n",
        "    # Perform Capacity Testing 1x for each size in range of AB_Union_min, AB_Union_max\n",
        "    for idx, size in enumerate(memory_AB_sizes):\n",
        "      num_interfering_memories = 0\n",
        "      Global_Interference_Level = 0.0\n",
        "      interference_capacity_reached = False\n",
        "      # Generate Memories and Evaluate Interference\n",
        "      while not interference_capacity_reached:\n",
        "        # Generate Memories\n",
        "        # Random subset of nodes for Memory AB\n",
        "        memory_AB = generate_random_indices_for_memory(node_to_idx_dict, size)\n",
        "        # Fire memory AB\n",
        "        activated_nodes, memory_C_size = count_firing_inputs(adj_matrix, memory_AB, k_s)\n",
        "\n",
        "        # Evaluate Interference between memory C and all other memories D\n",
        "        num_all_memories_D = len(all_trials[idx])\n",
        "        for mem in all_trials[idx]:\n",
        "          recurrence_nodes = np.intersect1d(activated_nodes, mem)\n",
        "          # Local interference found\n",
        "          if len(recurrence_nodes) / len(activated_nodes) >= LOCAL_INTERFERENCE_THRESHOLD:\n",
        "            num_interfering_memories += 2\n",
        "\n",
        "        # Check for global interference threshold (+1 is to count the last memory C generated in addition to the Memories D)\n",
        "        Global_Interference_Level = num_interfering_memories / (num_all_memories_D + 1)\n",
        "        if Global_Interference_Level >= GLOBAL_INTERFERENCE_THRESHOLD:\n",
        "          interference_capacity_reached = True\n",
        "          Global_Interference_Levels_per_AB_Size[idx] = Global_Interference_Level\n",
        "        else:\n",
        "          all_trials[idx].append(activated_nodes)\n",
        "\n",
        "      # Store this trial. Keep track of the number of memories for this Union(A,B) size AND the Global interference\n",
        "      stable_memories_per_AB_size[idx].append(len(all_trials[idx]))\n",
        "      Global_interference_per_AB_size[idx].append(Global_Interference_Levels_per_AB_Size[idx])\n",
        "\n",
        "\n",
        "  # Compute Mean and Standard Deviation of num stable memories, across all trials\n",
        "  stable_memories_sizes = [np.mean(vals) for vals in stable_memories_per_AB_size]\n",
        "  stable_memories_stds  = [np.std(vals) for vals in stable_memories_per_AB_size]  # or use np.std(vals)/np.sqrt(num_trials) for standard error\n",
        "\n",
        "  # Print Results from trials\n",
        "  zipped_trial_data = list(zip(memory_AB_sizes, Global_interference_per_AB_size, stable_memories_sizes))\n",
        "  print(\"AB_size, GLOBAL INTERFERENCE LEVEL, Number of Memories D:\")\n",
        "  for ab_size, global_level, memories in zipped_trial_data:\n",
        "    print(ab_size, np.mean(global_level), memories)\n",
        "\n",
        "  # Plot the mean line for error region\n",
        "  plt.plot(\n",
        "      memory_AB_sizes,\n",
        "      stable_memories_sizes,\n",
        "      '-o',\n",
        "      color='black',\n",
        "      markerfacecolor='gray',\n",
        "      label='Mean ± Std'\n",
        "  )\n",
        "\n",
        "  # Add the shaded error region\n",
        "  plt.fill_between(\n",
        "      memory_AB_sizes,\n",
        "      np.array(stable_memories_sizes) - np.array(stable_memories_stds),\n",
        "      np.array(stable_memories_sizes) + np.array(stable_memories_stds),\n",
        "      color='tomato',\n",
        "      alpha=0.3  # transparency of the shaded area\n",
        "  )\n",
        "\n",
        "  plt.xlabel(\"Union of Memory A and B Size\")\n",
        "  plt.ylabel(\"Number of Memories\\n at Capacity\", rotation=0, labelpad=50)\n",
        "  plt.title(f\"Union of Memory A and Memory B Size vs. The Number\\n of Memories at Capacity of {brain_region}\")\n",
        "  plt.grid(False)\n",
        "  plt.legend()\n",
        "  plt.savefig(f\"capacityChart_{graphType}.pdf\", format='pdf', bbox_inches='tight')\n",
        "  plt.show()\n",
        "  return memory_AB_sizes, stable_memories_sizes, stable_memories_stds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtwgWChTfJc5"
      },
      "outputs": [],
      "source": [
        "# CAP Test for JOIN on OCG\n",
        "\n",
        "# params specifically needed for 2-memory firing.\n",
        "r_min_per_mem_OCG = 100\n",
        "r_max_per_mem_OCG = 125\n",
        "n_OCG = 409\n",
        "# other params needed\n",
        "df_OCG = df[df[\"neuropil\"]=='OCG']\n",
        "brain_region_name_OCG = \"Brain Region OCG\"\n",
        "node_to_idx_dict_OCG = node_to_idx_OCG\n",
        "adj_matrix_OCG = adj_sparse_OCG\n",
        "\n",
        "# Fire memory A and memory B together, with varying sizes of 100-125 nodes each.\n",
        "OCG_memory_AB_sizes, OCG_stable_memories_sizes, OCG_stable_memories_stds = JOIN_Capacity_Simulation(df_OCG, r_min_per_mem_OCG, r_max_per_mem_OCG, n_OCG, brain_region_name_OCG, node_to_idx_dict_OCG, adj_matrix_OCG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhrAYj16fm41"
      },
      "source": [
        "### Capacity Testing on Watts-Strogatz graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5FWL9GKfmYv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "# Jointly fire Mem A and Mem B together to create Mem C. Use a one-step method by approximating the size union(A,B).\n",
        "def JOIN_WS_Capacity_Simulation(k_s_subgraph, r_min, r_max, n, brain_region_name, node_to_idx_dict, adj_matrix):\n",
        "  # Compute the size of Union(A,B) for r_min and r_max, respectively. 2 different sizes are calculated for a range of firing values.\n",
        "  union_AB_min_size = 2 * r_min - ((r_min ** 2) // n) - 1\n",
        "  union_AB_max_size = 2 * r_max - ((r_max ** 2) // n) - 1\n",
        "\n",
        "  # Generate memory A and fire to get memory C, then plot.\n",
        "  k_s = k_s_subgraph\n",
        "\n",
        "  memory_AB_size_MIN = union_AB_min_size  # different Union(A,B) sizes to test\n",
        "  memory_AB_size_MAX = union_AB_max_size  # different Union(A,B) sizes to test\n",
        "  memory_AB_sizes = []  # for plotting\n",
        "  memory_C_sizes = []  # for plotting\n",
        "\n",
        "  increment = 1\n",
        "  num_trials = 30\n",
        "\n",
        "  # Plot varying sizes of fired nodes in Memory A against firing nodes of created Memory C\n",
        "  memory_AB_sizes, stable_memories_sizes, stable_memories_stds = plot_memAB_firing_against_memC_Capacity_Testing_with_error_bars(memory_AB_sizes, memory_C_sizes, node_to_idx_dict, adj_matrix, k_s, memory_AB_size_MIN,\n",
        "                                        memory_AB_size_MAX, brain_region_name, \"WS\", increment, num_trials)\n",
        "  return memory_AB_sizes, stable_memories_sizes, stable_memories_stds\n",
        "\n",
        "\n",
        "# Generate a Watts–Strogatz small-world network adjacency matrix in sparse format.\n",
        "# n = Number of nodes\n",
        "# p = Rewiring probability\n",
        "# k = Each node is connected to k nearest neighbors in a ring lattice (graph degree/ for directed graph)\n",
        "# directed = Whether the graph is directed (True) or undirected (False)\n",
        "# seed = Random seed for reproducibility\n",
        "def create_ws_graph_sparse(n: int, p: float, knn: int = -1, directed: bool = True, seed=None):\n",
        "    if knn == -1:\n",
        "        knn = int((n + np.log(n)) // 2)\n",
        "    if knn > n:\n",
        "        raise ValueError(\"knn cannot be larger than n\")\n",
        "    if knn == n:\n",
        "        return coo_matrix(np.eye(n)).tocsr()\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Collect edges for sparse adj matrix\n",
        "    rows = []\n",
        "    cols = []\n",
        "\n",
        "    pool = rng.random(n * knn)\n",
        "    for pix, (n_ix, iy) in enumerate(itertools.product(range(n), range(1, knn + 1))):\n",
        "        idx = n_iy = (n_ix + iy) % n  # lattice neighbor\n",
        "        if pool[pix] < p:\n",
        "            # rewire each edge of k-nearest neighbors with probability p to a random node\n",
        "            while idx == n_ix or idx == n_iy:\n",
        "                idx = rng.integers(0, n)\n",
        "        rows.append(n_ix)\n",
        "        cols.append(idx)\n",
        "\n",
        "        if not directed:\n",
        "            rows.append(idx)\n",
        "            cols.append(n_ix)\n",
        "\n",
        "    data = np.ones(len(rows), dtype=int)\n",
        "    adj_sparse = coo_matrix((data, (rows, cols)), shape=(n, n)).tocsr()\n",
        "\n",
        "    # Make sure it stays binary (no multi-edges summing to >1)\n",
        "    adj_sparse.data[:] = 1\n",
        "\n",
        "    return adj_sparse\n",
        "\n",
        "# direct = Directed Boolean\n",
        "# n = Number of nodes\n",
        "# d = degree of biological graph (drosophila)\n",
        "# p = Edge probability\n",
        "def compute_p(n, d, directed):\n",
        "  if directed:\n",
        "    return d / (2 * (n - 1))\n",
        "  else:\n",
        "    return d / (n - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD5nqqfi1MM-"
      },
      "source": [
        "#### Try a range of n nodes to observe the change in interference and stable memories.\n",
        "\n",
        "\n",
        "\n",
        "*   n = 409\n",
        "*   d = 12\n",
        "\n",
        "Emulates the Stability of OCG over different AB Union sizes well.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEZFIuxU1P9c"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Try various n node sizes over a range of [380, 420] to understand the effect on\n",
        "varying_n_over_WS = [409]\n",
        "for n in varying_n_over_WS:\n",
        "  # Compute Adj_Matrix, Small-World Metrics for WS Graph using OCG Small-World Metrics\n",
        "  ws_OCG_n = n\n",
        "  ws_OCG_d = 12\n",
        "  ws_OCG_k = int(k_s_OCG)\n",
        "  print(\"k: \", ws_OCG_k)\n",
        "  ws_OCG_directed = True\n",
        "  ws_OCG_p = compute_p(ws_OCG_n, ws_OCG_d, ws_OCG_directed)\n",
        "  print(\"p: \", ws_OCG_p)\n",
        "  ws_OCG_knn = math.ceil(ws_OCG_d / 2)\n",
        "  ws_adj_matrix_OCG = create_ws_graph_sparse(ws_OCG_n, ws_OCG_p, ws_OCG_knn)\n",
        "\n",
        "  # Compute WS Small World Metrics\n",
        "  deg_dist_WS_OCG_IN, deg_dist_WS_OCG_OUT = calculate_degree_distribution(ws_adj_matrix_OCG, ws_OCG_directed)\n",
        "  print(\"In-Degree Distribution: \", np.mean(deg_dist_WS_OCG_IN))\n",
        "  print(\"Out-Degree Distribution: \", np.mean(deg_dist_WS_OCG_OUT))\n",
        "\n",
        "  # Compute Clustering Coefficient\n",
        "  clustCoeff_WS_OCG = calculate_global_clustering_sparse(ws_adj_matrix_OCG, ws_OCG_directed)\n",
        "  print(\"Clustering Coefficient: \", clustCoeff_WS_OCG)\n",
        "\n",
        "  # Compute Characteristic Path Length\n",
        "  characteristic_path_length_WS_OCG = calculate_path_length_sparse(ws_adj_matrix_OCG, ws_OCG_directed)\n",
        "  print(\"Path Length: \", characteristic_path_length_WS_OCG)\n",
        "\n",
        "  # Compute Average Degree of Graph\n",
        "  print(\"Average Degree of Graph:\", compute_graph_average_degree(ws_adj_matrix_OCG, ws_OCG_directed))\n",
        "\n",
        "  # Fire Memory A and Memory B in the WS Small Worlds Graph to simulate JOIN\n",
        "  r_min_per_mem_OCG = 100\n",
        "  r_max_per_mem_OCG = 125\n",
        "\n",
        "  # other params needed\n",
        "  ws_brain_region_name_OCG = \"WS Graph of OCG Qualities (n, d, k)\"\n",
        "  node_to_idx_dict_OCG = [0] * ws_OCG_n\n",
        "\n",
        "  # Fire memory A and memory B together, with varying sizes of 100-125 nodes each.\n",
        "  WS_memory_AB_sizes, WS_stable_memories_sizes, WS_stable_memories_stds = JOIN_WS_Capacity_Simulation(ws_OCG_k, r_min_per_mem_OCG, r_max_per_mem_OCG, ws_OCG_n, ws_brain_region_name_OCG, node_to_idx_dict_OCG, ws_adj_matrix_OCG)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot OCG and WS Capacity Distributions on Top of One Another"
      ],
      "metadata": {
        "id": "IZjWgUEHdW08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def JOIN_BOTH_Capacity_Simulation(df_OCG, r_min_per_mem_OCG, r_max_per_mem_OCG, n_OCG, brain_region_name_OCG, node_to_idx_dict_OCG, adj_matrix_OCG,\n",
        "    ws_OCG_k, ws_OCG_n, ws_brain_region_name_OCG, ws_adj_matrix_OCG):\n",
        "  OCG_memory_AB_sizes, OCG_stable_memories_sizes, OCG_stable_memories_stds = JOIN_Capacity_Simulation(df_OCG, r_min_per_mem_OCG, r_max_per_mem_OCG, n_OCG, brain_region_name_OCG, node_to_idx_dict_OCG, adj_matrix_OCG)\n",
        "  WS_memory_AB_sizes, WS_stable_memories_sizes, WS_stable_memories_stds = JOIN_WS_Capacity_Simulation(ws_OCG_k, r_min_per_mem_OCG, r_max_per_mem_OCG, ws_OCG_n, ws_brain_region_name_OCG, node_to_idx_dict_OCG, ws_adj_matrix_OCG)\n",
        "\n",
        "  ###### PLOT OCG ######\n",
        "  # Plot the mean line for error region\n",
        "  plt.plot(\n",
        "      OCG_memory_AB_sizes,\n",
        "      OCG_stable_memories_sizes,\n",
        "      '-o',\n",
        "      color='black',\n",
        "      markerfacecolor='red',\n",
        "      label='OCG Mean ± Std'\n",
        "  )\n",
        "\n",
        "  # Add the shaded error region\n",
        "  plt.fill_between(\n",
        "      OCG_memory_AB_sizes,\n",
        "      np.array(OCG_stable_memories_sizes) - np.array(OCG_stable_memories_stds),\n",
        "      np.array(OCG_stable_memories_sizes) + np.array(OCG_stable_memories_stds),\n",
        "      color='tomato',\n",
        "      alpha=0.3  # transparency of the shaded area\n",
        "  )\n",
        "\n",
        "  ###### PLOT WS ######\n",
        "  # Plot the mean line for error region\n",
        "  plt.plot(\n",
        "      WS_memory_AB_sizes,\n",
        "      WS_stable_memories_sizes,\n",
        "      '-o',\n",
        "      color='black',\n",
        "      markerfacecolor='blue',\n",
        "      label='WS Mean ± Std'\n",
        "  )\n",
        "\n",
        "  # Add the shaded error region\n",
        "  plt.fill_between(\n",
        "      WS_memory_AB_sizes,\n",
        "      np.array(WS_stable_memories_sizes) - np.array(WS_stable_memories_stds),\n",
        "      np.array(WS_stable_memories_sizes) + np.array(WS_stable_memories_stds),\n",
        "      color='blue',\n",
        "      alpha=0.3  # transparency of the shaded area\n",
        "  )\n",
        "\n",
        "  plt.xlabel(\"Union of Memory A and B Size\")\n",
        "  plt.ylabel(\"Number of Memories\\n at Capacity\", rotation=0, labelpad=60)\n",
        "  plt.title(f\"Union of Memory A and Memory B Size vs. The Number\\n of Memories at Capacity Between OCG and WS\")\n",
        "  plt.grid(False)\n",
        "  plt.legend()\n",
        "  plt.savefig(f\"capacityChart_Overlay.pdf\", format='pdf', bbox_inches='tight')\n",
        "  plt.show()\n",
        "\n",
        "JOIN_BOTH_Capacity_Simulation(df_OCG, r_min_per_mem_OCG, r_max_per_mem_OCG, n_OCG, brain_region_name_OCG, node_to_idx_dict_OCG, adj_matrix_OCG,\n",
        "    ws_OCG_k, ws_OCG_n, ws_brain_region_name_OCG, ws_adj_matrix_OCG)\n",
        "\n"
      ],
      "metadata": {
        "id": "_TGuYFTEdqO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2PV0SCg4Fq6"
      },
      "source": [
        "#### Make a WS Graph with tested interference and stability ranges. Then test to verify the WS Graph meets small world metrics.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Watts-Strogatz is much more uniform due to the way the nodes are connected to each other than the real world brain connections of OCG. It is possible that OCG has more threshold for misfires, as the graph is not completely connected. Thus, we would not need as many edges to achieve the same connectivity. This is why we have a smaller **d**.\n",
        "*   n is 400 vs. 409 of OCG. This is a difference of ~10 nodes. Not a large consideration.\n",
        "\n",
        "\n",
        "MemA and MemB Union size of [200, 210] ==> 30 trials\n",
        "Use error bars with mean and st dev."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DQmfc8ufW4u"
      },
      "outputs": [],
      "source": [
        "# Compute Adj_Matrix, Small-World Metrics for WS Graph using OCG Small-World Metrics\n",
        "import math\n",
        "ws_OCG_n = 409\n",
        "ws_OCG_d = 12\n",
        "ws_OCG_k = int(k_s_OCG)\n",
        "print(\"k: \", ws_OCG_k)\n",
        "\n",
        "ws_OCG_directed = True\n",
        "ws_OCG_p = compute_p(ws_OCG_n, ws_OCG_d, ws_OCG_directed)\n",
        "print(\"p: \", ws_OCG_p)\n",
        "\n",
        "ws_OCG_knn = math.ceil(ws_OCG_d / 2)\n",
        "ws_adj_matrix_OCG = create_ws_graph_sparse(ws_OCG_n, ws_OCG_p, ws_OCG_knn)\n",
        "\n",
        "# Compute WS Small World Metrics\n",
        "deg_dist_WS_OCG_IN, deg_dist_WS_OCG_OUT = calculate_degree_distribution(ws_adj_matrix_OCG, ws_OCG_directed)\n",
        "print(\"In-Degree Distribution: \", np.mean(deg_dist_WS_OCG_IN))\n",
        "print(\"Out-Degree Distribution: \", np.mean(deg_dist_WS_OCG_OUT))\n",
        "\n",
        "# Compute Clustering Coefficient\n",
        "clustCoeff_WS_OCG = calculate_global_clustering_sparse(ws_adj_matrix_OCG, ws_OCG_directed)\n",
        "print(\"Clustering Coefficient: \", clustCoeff_WS_OCG)\n",
        "\n",
        "# Compute Characteristic Path Length\n",
        "characteristic_path_length_WS_OCG = calculate_path_length_sparse(ws_adj_matrix_OCG, ws_OCG_directed)\n",
        "print(\"Path Length: \", characteristic_path_length_WS_OCG)\n",
        "\n",
        "# Compute Average Degree of Graph\n",
        "graph_average_degree_WS_OCG = compute_graph_average_degree(ws_adj_matrix_OCG, ws_OCG_directed)\n",
        "print(\"Average Degree of Graph:\", graph_average_degree_WS_OCG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JerVl3xEJkUQ"
      },
      "outputs": [],
      "source": [
        "# Small World Test on WS Graph with Above n,d,k\n",
        "\n",
        "# Find Graph Average Degree to use in random Gnp graph calculation\n",
        "print(compute_graph_average_degree(ws_adj_matrix_OCG, ws_OCG_directed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hee3TW9mJ4WK"
      },
      "outputs": [],
      "source": [
        "# import C_rand, L_rand values from randGraph.ipynb\n",
        "C_rand_WS = 0.015730598927988813\n",
        "L_rand_WS = 3.589833550913838\n",
        "S_WS = compute_small_world_threshold(clustCoeff_WS_OCG, characteristic_path_length_WS_OCG, C_rand, L_rand)\n",
        "print(S_WS)\n",
        "\n",
        "if S_WS > 1:\n",
        "  print(\"Small world found!\")\n",
        "else:\n",
        "  print(\"Not a small world...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4voTkL3SVtEA"
      },
      "outputs": [],
      "source": [
        "# Fire Memory A and Memory B in the WS Small Worlds Graph to simulate JOIN\n",
        "r_min_per_mem_OCG = 117\n",
        "r_max_per_mem_OCG = 124\n",
        "\n",
        "# other params needed\n",
        "ws_brain_region_name_OCG = \"WS of OCG Qualities (n, d, k)\"\n",
        "node_to_idx_dict_OCG = [0] * ws_OCG_n\n",
        "\n",
        "# Fire memory A and memory B together, with varying sizes of 100-125 nodes each.\n",
        "JOIN_WS_Capacity_Simulation(ws_OCG_k, r_min_per_mem_OCG, r_max_per_mem_OCG, ws_OCG_n, ws_brain_region_name_OCG, node_to_idx_dict_OCG, ws_adj_matrix_OCG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kLERvvQ4I9v"
      },
      "source": [
        "## Visualize Brain Region Subgraphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs2QTN6O4OOg"
      },
      "source": [
        "### OCG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aFhIZC38JJI"
      },
      "outputs": [],
      "source": [
        "# OVERALL OCG GRAPH\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Take adj_sparse (CSR matrix)\n",
        "# Convert sparse matrix to a NetworkX Graph\n",
        "G = nx.from_scipy_sparse_array(adj_matrix_OCG, create_using=nx.DiGraph)\n",
        "pos = nx.spring_layout(G, seed=0)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(8, 8))\n",
        "nx.draw(\n",
        "    G,\n",
        "    node_size=50,          # smaller nodes\n",
        "    node_color=\"black\",    # black nodes\n",
        "    edge_color=\"gray\",     # gray edges\n",
        "    width=0.5,\n",
        "    alpha=0.7,\n",
        "    with_labels=False      # turn on if you want node labels\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH9uHodc4Hxs"
      },
      "outputs": [],
      "source": [
        "# ZOOMED IN VIEW ON CENTRAL CLUSTER OF NODES\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you already have adj_sparse from your function (CSR matrix)\n",
        "# Convert sparse matrix to a NetworkX Graph\n",
        "G = nx.from_scipy_sparse_array(adj_matrix_OCG, create_using=nx.DiGraph)\n",
        "pos = nx.spring_layout(G, seed=0)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(12, 12))\n",
        "nx.draw(\n",
        "    G,\n",
        "    node_size=50,          # smaller nodes\n",
        "    node_color=\"black\",    # black nodes\n",
        "    edge_color=\"gray\",     # gray edges\n",
        "    width=0.5,\n",
        "    alpha=0.7,\n",
        "    with_labels=False      # turn on if you want node labels\n",
        ")\n",
        "\n",
        "# ZOOM IN ON THE LARGE CENTRAL CLUSTER\n",
        "plt.xlim(-0.1, 0.15)   # tighter zoom horizontally\n",
        "plt.ylim(-0.1, 0.1)   # tighter zoom vertically\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WPFn_6E9YBp"
      },
      "outputs": [],
      "source": [
        "# ZOOMED IN VIEW ON CENTRAL CLUSTER OF NODES\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you already have adj_sparse from your function (CSR matrix)\n",
        "# Convert sparse matrix to a NetworkX Graph\n",
        "G = nx.from_scipy_sparse_array(adj_matrix_OCG, create_using=nx.DiGraph)\n",
        "pos = nx.spring_layout(G, seed=0)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(12, 12))\n",
        "nx.draw(\n",
        "    G,\n",
        "    node_size=50,          # smaller nodes\n",
        "    node_color=\"black\",    # black nodes\n",
        "    edge_color=\"gray\",     # gray edges\n",
        "    width=0.5,\n",
        "    alpha=0.7,\n",
        "    with_labels=True,\n",
        "    font_size=15,\n",
        "    font_color=\"red\"\n",
        ")\n",
        "\n",
        "# # Add labels separately with an offset (d_y shift upwards)\n",
        "# labels = {n: str(n) for n in G.nodes()}\n",
        "# label_pos = {n: (x, y + 0.00000001) for n, (x, y) in pos.items()}\n",
        "\n",
        "# nx.draw_networkx_labels(\n",
        "#     G,\n",
        "#     label_pos,\n",
        "#     labels,\n",
        "#     font_size=8,\n",
        "#     font_color=\"red\"   # choose a contrasting color\n",
        "# )\n",
        "\n",
        "# ZOOM IN ON THE LARGE CENTRAL CLUSTER\n",
        "plt.xlim(-0.1, 0.15)   # tighter zoom horizontally\n",
        "plt.ylim(-0.1, 0.1)   # tighter zoom vertically\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3r5gbtwiK1i"
      },
      "outputs": [],
      "source": [
        "# OVERALL LH_L GRAPH\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Take adj_sparse (CSR matrix)\n",
        "# Convert sparse matrix to a NetworkX Graph\n",
        "G = nx.from_scipy_sparse_array(adj_matrix, create_using=nx.DiGraph)\n",
        "pos = nx.spring_layout(G, seed=0)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(8, 8))\n",
        "nx.draw(\n",
        "    G,\n",
        "    node_size=50,          # smaller nodes\n",
        "    node_color=\"black\",    # black nodes\n",
        "    edge_color=\"gray\",     # gray edges\n",
        "    width=0.5,\n",
        "    alpha=0.7,\n",
        "    with_labels=False      # turn on if you want node labels\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It5VpcNGpEDR"
      },
      "outputs": [],
      "source": [
        "# ZOOMED IN VIEW ON CENTRAL CLUSTER OF NODES\n",
        "\n",
        "# Convert sparse matrix to a NetworkX Graph\n",
        "G = nx.from_scipy_sparse_array(adj_matrix, create_using=nx.DiGraph)\n",
        "pos = nx.spring_layout(G, seed=0)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(12, 12))\n",
        "nx.draw(\n",
        "    G,\n",
        "    node_size=50,          # smaller nodes\n",
        "    node_color=\"black\",    # black nodes\n",
        "    edge_color=\"gray\",     # gray edges\n",
        "    width=0.5,\n",
        "    alpha=0.7,\n",
        "    with_labels=False      # turn on if you want node labels\n",
        ")\n",
        "\n",
        "# ZOOM IN ON THE LARGE CENTRAL CLUSTER\n",
        "plt.xlim(-0.35, 0.3)   # tighter zoom horizontally\n",
        "plt.ylim(-0.3, 0.35)   # tighter zoom vertically\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "icZVORlEdlAf",
        "Cr2KFQbyxArC",
        "l8lYgr2s8VoQ",
        "L8SHJT9A8Zeh",
        "6kLERvvQ4I9v"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}